{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence and Word Segmentation\n",
    "\n",
    "The first step in NLP is cutting text into its constituents. Namely, sentences and words. Let's see how well we can perform this task in base python.\n",
    "\n",
    "**DO NOT worry about writing efficient code.** We're just practicing NLP principles.\n",
    "\n",
    "It will useful to know the String methods! These are one of the most useful features of Python for text processing!\n",
    "\n",
    "https://docs.python.org/2/library/stdtypes.html#string-formatting-operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation\n",
    "\n",
    "Let's start with sentence segmentation. English typically end with a period, exclamation, or question mark. Let's start easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keep a list of sentences, and a temp string with the current sentence. Append when you hit the right characters'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell for a HINT:\n",
    "import base64\n",
    "base64.decodestring('S2VlcCBhIGxpc3Qgb2Ygc2VudGVuY2VzLCBhbmQgYSB0ZW1wIHN0cmluZyB3aXRoIHRoZSBjdXJy\\nZW50IHNlbnRlbmNlLiBBcHBlbmQgd2hlbiB5b3UgaGl0IHRoZSByaWdodCBjaGFyYWN0ZXJz\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the input text and what the output should be.\n",
    "\n",
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "easy_split_text = [\"I went to the zoo today.\",\n",
    "                   \"What do you think of that?\",\n",
    "                   \"I bet you hate it!\",\n",
    "                   \"Or maybe you don't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function to split a string into sentences.\n",
    "# If you're familiar with regexes, feel free to use the re module\n",
    "\n",
    "def sentencer(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "    sentences = []\n",
    "    substring = ' '\n",
    "    for c in text:\n",
    "            substring += c\n",
    "            if c in ['.','?','!']:\n",
    "                sentences.append(str.strip(substring + \" \\t\"))\n",
    "                substring = ''\n",
    "    sentences.append(substring)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if map(str.strip, sentencer(easy_text)) == easy_split_text:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print sentencer(easy_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print easy_split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation continued\n",
    "\n",
    "What about cases where periods denote abbreviations? This time, try to do the same splits, but accommodate 'Dr.', 'Mrs.', 'Mr.', and 'Ms.'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the input text and what the output should be.\n",
    "\n",
    "med_text = \"My name is Dr. Lee. There is also a Mrs. Lee. Actually, there are tons! They're other people's wives.\"\n",
    "med_split_text = [\"My name is Dr. Lee.\",\n",
    "                  \"There is also a Mrs. Lee.\",\n",
    "                  \"Actually, there are tons!\",\n",
    "                  \"They're other people's wives.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify your last sentencer to account for these new patterns.\n",
    "\n",
    "\n",
    "def sentencer2(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "    sentences = []\n",
    "    substring = ' '\n",
    "    for c in text:\n",
    "            substring += c\n",
    "            if c in ['?','!']:\n",
    "                sentences.append(str.strip(substring + \" \\t\"))\n",
    "                substring = ''\n",
    "                     \n",
    "            if c in ['.']:\n",
    "                if substring[-4:] != 'Mrs.' & substring[-3:]!='Dr.':\n",
    "                    sentences.append(str.strip(substring + \" \\t\"))\n",
    "                    substring = '' \n",
    "    sentences.append(substring)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b7932a1b2a54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test your function by running this cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentencer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmed_split_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Congratulations!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-784341d7e540>\u001b[0m in \u001b[0;36msentencer2\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0msubstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Mrs.'\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0msubstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'Dr.'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstring\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0msubstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if map(str.strip, sentencer2(med_text)) == med_split_text:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print sentencer2(med_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print med_split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Home Exercise: sentence segmentation continued\n",
    "\n",
    "Abbreviations like 'a.k.a.' are harder to accommodate. This one is quite challenging, so you can skip it if you want to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell for a HINT:\n",
    "import base64\n",
    "base64.decodestring('VHJ5IGFsbG93aW5nIHRoZSBzcGxpdHMgb24gdGhlIHBlcmlvZHMsIGJ1dCB0aGVuIHJlYXR0YWNo\\naW5nIGlmIHRoZSBuZXh0IHNlbnRlbmNlIGlzIG9ubHkgb25lIGNoYXJhY3RlciBsb25n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the input text and what the output should be.\n",
    "\n",
    "hard_text = \"I know an M.D., i.e. a doctor. Like Dr. Smith, a.k.a. Docsmith.\"\n",
    "hard_split_text = [\"I know an M.D., i.e. a doctor.\",\n",
    "                   \"Like Dr. Smith, a.k.a. Docsmith.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take home exercise:\n",
    "# modify your last sentencer to account for these new patterns.\n",
    "\n",
    "def sentencer3(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "    # FILL IN CODE\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if map(str.strip, sentencer3(hard_text)) == hard_split_text:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print sentencer3(hard_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print hard_split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation continued\n",
    "\n",
    "Sentence segmentation is harder than it seems! Let's take a look at how a modern system does it. [NLTK](http://www.nltk.org) is the most widely-used NLP library in Python. It [relies on a statistical language model](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt) to determine when to split sentences. You'll notice that even this model can't handle our hard sentences.\n",
    "\n",
    "To get started, you have to download the right dataset. **DO NOT** download everything. It will take forever. When the download window pops up (probably behind your other windows, annoyingly) click on the 'Models' tab, choose the 'punkt' dataset, and just download that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a864f0d22c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"did you read the instructions?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rutwang/anaconda/lib/python2.7/site-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;31m# function should make a new copy of self to use?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rutwang/anaconda/lib/python2.7/site-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36m_interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mTKINTER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                 \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rutwang/anaconda/lib/python2.7/site-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[0;31m#/////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rutwang/anaconda/lib/python2.7/lib-tk/Tkinter.pyc\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# download the Punkt Tokenizer Models.\n",
    "# DON'T DOWNLOAD EVERYTHING!\n",
    "# The download window will probably pop up behind your other windows.\n",
    "# uncomment the download command and comment out the print statement when you've understood these instructions.\n",
    "\n",
    "import nltk\n",
    "nltk.download()\n",
    "print \"did you read the instructions?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I went to the zoo today.', 'What do you think of that?', 'I bet you hate it!', \"Or maybe you don't\"]\n",
      "['My name is Dr. Lee.', 'There is also a Mrs. Lee.', 'Actually, there are tons!', \"They're other people's wives.\"]\n",
      "['I know an M.D., i.e.', 'a doctor.', 'Like Dr. Smith, a.k.a.', 'Docsmith.']\n"
     ]
    }
   ],
   "source": [
    "print sent_detector.sentences_from_text(easy_text)\n",
    "print sent_detector.sentences_from_text(med_text)\n",
    "print sent_detector.sentences_from_text(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenization\n",
    "\n",
    "A more common task is to ignore sentences and just split text into words. We call this tokenization. Try your hand at this. This task is much easier now that you're familiar with all the string methods. right?? You should be able to write a fairly simple function that can tokenize all of our texts from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our objective tokenizations. Note that we've removed some punctuation.\n",
    "easy_words = ['I', 'went', 'to', 'the', 'zoo', 'today',\n",
    "              'What', 'do', 'you', 'think', 'of', 'that',\n",
    "              'I', 'bet', 'you', 'hate', 'it',\n",
    "              'Or', 'maybe', 'you', \"don't\"]\n",
    "med_words = ['My', 'name', 'is', 'Dr', 'Lee',\n",
    "             'There', 'is', 'also', 'a', 'Mrs', 'Lee',\n",
    "             'Actually,', 'there', 'are', 'tons',\n",
    "             \"They're\", 'other', \"people's\", 'wives']\n",
    "hard_words = ['I', 'know', 'an', 'MD,', 'ie', 'a', 'doctor',\n",
    "              'Like', 'Dr', 'Smith,', 'aka', 'Docsmith']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function to split a string into sentences.\n",
    "# If you're familiar with regexes, feel free to use the re module\n",
    "\n",
    "def tokenizer(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a WORD'''\n",
    "    cleaned_text = text.replace('.',' ').replace('?', ' ').replace('!', ' ')\n",
    "    words = cleaned_text.split()\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(easy_text) == easy_words:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print tokenizer(easy_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print easy_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(med_text) == med_words:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print tokenizer(med_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print med_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, try again!\n",
      "\n",
      "Your version:\n",
      "['I', 'know', 'an', 'M', 'D', ',', 'i', 'e', 'a', 'doctor', 'Like', 'Dr', 'Smith,', 'a', 'k', 'a', 'Docsmith']\n",
      "\n",
      "Desired output:\n",
      "['I', 'know', 'an', 'MD,', 'ie', 'a', 'doctor', 'Like', 'Dr', 'Smith,', 'aka', 'Docsmith']\n"
     ]
    }
   ],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(hard_text) == hard_words:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "\n",
    "    print tokenizer(hard_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print hard_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization continued\n",
    "\n",
    "Let's see how NLTK [tokenizes text into words](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'went', 'to', 'the', 'zoo', 'today', '.', 'What', 'do', 'you', 'think', 'of', 'that', '?', 'I', 'bet', 'you', 'hate', 'it', '!', 'Or', 'maybe', 'you', 'do', \"n't\"]\n",
      "['My', 'name', 'is', 'Dr.', 'Lee', '.', 'There', 'is', 'also', 'a', 'Mrs.', 'Lee', '.', 'Actually', ',', 'there', 'are', 'tons', '!', 'They', \"'re\", 'other', 'people', \"'s\", 'wives', '.']\n",
      "['I', 'know', 'an', 'M.D.', ',', 'i.e', '.', 'a', 'doctor', '.', 'Like', 'Dr.', 'Smith', ',', 'a.k.a', '.', 'Docsmith', '.']\n"
     ]
    }
   ],
   "source": [
    "print word_tokenize(easy_text)\n",
    "print word_tokenize(med_text)\n",
    "print word_tokenize(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It behaves a little differently, and sometimes erratically. Let's try a version based on pattern-matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'went', 'to', 'the', 'zoo', 'today', '.', 'What', 'do', 'you', 'think', 'of', 'that', '?', 'I', 'bet', 'you', 'hate', 'it', '!', 'Or', 'maybe', 'you', 'don', \"'\", 't']\n",
      "['My', 'name', 'is', 'Dr', '.', 'Lee', '.', 'There', 'is', 'also', 'a', 'Mrs', '.', 'Lee', '.', 'Actually', ',', 'there', 'are', 'tons', '!', 'They', \"'\", 're', 'other', 'people', \"'\", 's', 'wives', '.']\n",
      "['I', 'know', 'an', 'M', '.', 'D', '.,', 'i', '.', 'e', '.', 'a', 'doctor', '.', 'Like', 'Dr', '.', 'Smith', ',', 'a', '.', 'k', '.', 'a', '.', 'Docsmith', '.']\n"
     ]
    }
   ],
   "source": [
    "print wordpunct_tokenize(easy_text)\n",
    "print wordpunct_tokenize(med_text)\n",
    "print wordpunct_tokenize(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, there isn't really a right or wrong way to tokenize words. Sometimes punctuation provides valuable semantic content. Sometimes, you want to strip it all away.\n",
    "\n",
    "As a final thought, what do you suppose the following functions do? Go ahead and play with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'went')\n",
      "('went', 'to')\n",
      "('to', 'the')\n",
      "('the', 'zoo')\n",
      "('zoo', 'today')\n",
      "('today', '.')\n",
      "('.', 'What')\n",
      "('What', 'do')\n",
      "('do', 'you')\n",
      "('you', 'think')\n",
      "('think', 'of')\n",
      "('of', 'that')\n",
      "('that', '?')\n",
      "('?', 'I')\n",
      "('I', 'bet')\n",
      "('bet', 'you')\n",
      "('you', 'hate')\n",
      "('hate', 'it')\n",
      "('it', '!')\n",
      "('!', 'Or')\n",
      "('Or', 'maybe')\n",
      "('maybe', 'you')\n",
      "('you', 'do')\n",
      "('do', \"n't\")\n"
     ]
    }
   ],
   "source": [
    "for bigram in bigrams(word_tokenize(easy_text)):\n",
    "    print bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
